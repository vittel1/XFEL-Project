{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original at https://git.xfel.eu/gitlab/dataAnalysis/calibration-services/blob/dev/calibration/processor/operations.py\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "from functools import partial\n",
    "import multiprocessing as mp\n",
    "import os.path as osp\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "from iminuit import Minuit\n",
    "from karabo_data import DataCollection, by_index, H5File\n",
    "\n",
    "from fit_functions import least_squares_np\n",
    "from utils import pulse_filter, parse_ids, find_proposal\n",
    "\n",
    "import dask.array as da\n",
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalHistogram:\n",
    "    \"\"\"Class to evaluate histogram\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    modno: (int) Channel number or module number\n",
    "    path: (str) Path to Run folder\n",
    "    dettype: (str) AGIPD, LPD\n",
    "    pixel_hist: (bool) optional\n",
    "        Default: False. For pixel wise histogram set it to True\"\"\"\n",
    "    def __init__(self, modno, path, dettype, pixel_hist=False):\n",
    "        \"\"\"Initialization\"\"\"\n",
    "        self.histograms = None\n",
    "        self.bin_edges = None\n",
    "        self.mean_image = None\n",
    "        self.fit_params = None\n",
    "\n",
    "        self.modno = modno\n",
    "        self.path = path\n",
    "        self.pixel_hist = pixel_hist\n",
    "        self.dettype = dettype\n",
    "        assert self.dettype in [\"AGIPD\", \"LPD\"]\n",
    "\n",
    "    def process(self, bin_edges, pulse_ids=None, workers=None, dark_run=None):\n",
    "        \"\"\"Evaluate Histogram and mean image\n",
    "        Parameters:\n",
    "        -----------\n",
    "            bin_edges: (np.ndarray) required\n",
    "            pulse_ids: str, optional\n",
    "                Default: all pulses \":\"\n",
    "                For eg. \":\" to select all pulses in a train\n",
    "                \"start:stop:step\" to select indices with certain step size\n",
    "                \"1,2,3\" comma separated pulse index to select specific pulses\n",
    "                \"1,2,3, 5:10\" mix of above two\n",
    "            workers: (int), optional.\n",
    "                Default: half of total cpus available\n",
    "                Distribute sequence files over multiple processors\n",
    "            dark_run: (numpy.ndarray) optional\n",
    "                dark_dta shape (n_pulses, slow_scan, fast_scan)\n",
    "                Default: None,\n",
    "                If provided dark data will be subtracted from images\n",
    "        \"\"\"\n",
    "        self.bin_edges = bin_edges\n",
    "        \n",
    "        self.dark_run = da.from_array(dark_run)\n",
    "        \n",
    "        pulse_ids = \":\" if pulse_ids is None else pulse_ids\n",
    "        self.pulses = parse_ids(pulse_ids)\n",
    "\n",
    "        if not self.path or self.modno not in range(16):\n",
    "            return\n",
    "\n",
    "\n",
    "        pattern = f\"(.+){self.dettype}{self.modno:02d}-S(.+).h5\"\n",
    "\n",
    "        sequences = [osp.join(self.path, f) for f in os.listdir(self.path)\n",
    "                     if f.endswith('.h5') and re.match(pattern, f)]\n",
    "\n",
    "\n",
    "        histograms = []\n",
    "        images = []\n",
    "        \n",
    "        for s in sequences:\n",
    "            image, hist = self._eval(s)\n",
    "            if image is not None and hist is not None:\n",
    "                histograms.append(hist)\n",
    "                images.append(image)\n",
    "\n",
    "        histogramsN, imagesN = compute(histograms, images)\n",
    "        \n",
    "        if imagesN and histogramsN:\n",
    "            self.mean_image = np.mean(np.stack(imagesN), axis=0)\n",
    "            self.histograms = sum(histogramsN)\n",
    "\n",
    "    def _eval(self, seq_file):\n",
    "        \"\"\"Histogram over all or individual pixels\"\"\"\n",
    "        if not seq_file:\n",
    "            return\n",
    "        run = H5File(seq_file).select_trains(by_index[:20])\n",
    "\n",
    "        module = [key for key in run.instrument_sources\n",
    "                  if re.match(r\"(.+)/DET/(.+):(.+)\", key)]\n",
    "\n",
    "        if len(module) != 1:\n",
    "            return\n",
    "\n",
    "        histogram = 0\n",
    "        mean_image = 0\n",
    "        train_counts = 0\n",
    "\n",
    "        for tid, data in run.trains(devices=[(module[0], \"image.data\")],\n",
    "            require_all=True):\n",
    "\n",
    "            \n",
    "            image = da.from_array(data[module[0]][\"image.data\"][:, 0, ...])\n",
    "            print(image)\n",
    "\n",
    "            if image.shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            if self.pulses != [-1]:\n",
    "                image = image[self.pulses, ...].astype(np.float32)\n",
    "            else:\n",
    "                image = image.astype(np.float32)\n",
    "\n",
    "            if self.dark_run is not None:\n",
    "                dark_data = self.dark_run\n",
    "                if image.shape == dark_data.shape:\n",
    "                    image -= dark_data\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        f\"Different data shapes, dark_data: {dark_data.shape}\"\n",
    "                        f\" Run data: {image.shape}\")\n",
    "\n",
    "            mean_image += image\n",
    "            print(\"mean image: \", mean_image)\n",
    "            train_counts += 1\n",
    "\n",
    "            if not self.pixel_hist:\n",
    "                \"\"\"Evaluate histogram over entire module\"\"\"\n",
    "                counts_pr = []\n",
    "\n",
    "                def _eval_stat(pulse):\n",
    "                    counts, _ = da.histogram(\n",
    "                        image[pulse, ...].ravel(), bins=self.bin_edges)\n",
    "                    return counts\n",
    "\n",
    "                for i in range(image.shape[0]):\n",
    "                    counts_pr.append(self._eval_stat(i))\n",
    "                histogram += da.stack(counts_pr)\n",
    "\n",
    "            else:\n",
    "                \"\"\"Evaluate histogram over each pixel\"\"\"\n",
    "                def multihist(image, bin_edges):\n",
    "                    data = []\n",
    "                    for i in range(image.shape[1]):\n",
    "                        for j in range(image.shape[2]):\n",
    "                            histForOnePixel, _ = da.histogram(image[:,i,j].ravel(), bins=bin_edges)\n",
    "                            data.append(histForOnePixel)\n",
    "                    \n",
    "                    retTwoD = da.concatenate(data, axis=0)\n",
    "                    print(\"retTwoD shape: \", retTwoD.shape)\n",
    "                    ret = retTwoD.reshape(-1, image.shape[1], image.shape[2])\n",
    "                    print(\"Shape after hist: \", ret.shape)\n",
    "                    return ret\n",
    "                               \n",
    "                            \n",
    "                \n",
    "                counts = multihist(image, self.bin_edges)\n",
    "                    \n",
    "\n",
    "                histogram += counts\n",
    "\n",
    "\n",
    "        print(\"Total \", histogram.shape)\n",
    "        if train_counts != 0:\n",
    "            mean = mean_image / train_counts\n",
    "            return mean, histogram\n",
    "\n",
    "    def hist_to_file(self, path):\n",
    "        \"\"\"Write histograms to H5 File\"\"\"\n",
    "        if all([\n",
    "            self.histograms is not None,\n",
    "            self.mean_image is not None,\n",
    "            path]):\n",
    "\n",
    "            bin_centers = (self.bin_edges[1:] + self.bin_edges[:-1]) / 2.0\n",
    "            with h5py.File(path, \"w\") as f:\n",
    "                g = f.create_group(f\"entry_1/instrument/module_{self.modno}\")\n",
    "                g.create_dataset('counts', data=self.histograms)\n",
    "                g.create_dataset('bins', data=bin_centers)\n",
    "                g.create_dataset('image', data=self.mean_image)\n",
    "\n",
    "    def fit_histogram(self, init_params, bounds_params,\n",
    "                      from_file=None, threshold=(-50, 120)):\n",
    "\n",
    "        histogram = self.histograms\n",
    "        bin_edges = self.bin_edges\n",
    "\n",
    "        if bin_edges is not None:\n",
    "            bin_centers = (self.bin_edges[1:] + self.bin_edges[:-1]) / 2.0\n",
    "\n",
    "        self.init_params = init_params\n",
    "        self.bounds_params = bounds_params\n",
    "        if from_file is not None:\n",
    "            with h5py.File(from_file, \"r\") as f:\n",
    "                bin_centers = \\\n",
    "                    f[f\"entry_1/instrument/module_{self.modno}/bins\"][:]\n",
    "                histogram = \\\n",
    "                    f[f\"entry_1/instrument/module_{self.modno}/counts\"][:]\n",
    "\n",
    "        if histogram is None:\n",
    "            return\n",
    "\n",
    "        low, high = threshold\n",
    "        idx = (bin_centers > low) & (bin_centers < high)\n",
    "\n",
    "        hist_for_each = np.split(histogram.flatten(),\n",
    "                                 np.product(histogram.shape[:-1]))\n",
    "\n",
    "        map_fitting = partial(self._fitting, idx, bin_centers)\n",
    "        \n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "            ret = executor.map(map_fitting, hist_for_each)\n",
    "\n",
    "        #ret = map(map_fitting, hist_for_each)\n",
    "\n",
    "        self.fit_params = np.array(\n",
    "            list(ret)).reshape(\n",
    "            histogram.shape[:-1]+(2*len(self.init_params)+1,))\n",
    "\n",
    "    def _fitting(self, idx, bin_centers, histogram):\n",
    "        least_sq = partial(\n",
    "            least_squares_np,\n",
    "            bin_centers[idx],\n",
    "            histogram[idx])\n",
    "\n",
    "        m = Minuit.from_array_func(\n",
    "            least_sq,\n",
    "            self.init_params,\n",
    "            error=0.1,\n",
    "            errordef=1,\n",
    "            limit=tuple(self.bounds_params))\n",
    "        \n",
    "        \n",
    "        minuit_res = m.migrad()\n",
    "        \n",
    "        \n",
    "        return np.concatenate(\n",
    "            (m.np_values(),\n",
    "             m.np_errors(),\n",
    "             np.array([m.get_fmin().is_valid])))\n",
    "\n",
    "    def fit_params_to_file(self, path):\n",
    "        \"\"\"Write fit params to H5 File\"\"\"\n",
    "        if all([self.fit_params is not None, path]):\n",
    "            with h5py.File(path, \"w\") as f:\n",
    "                g = f.create_group(f\"entry_1/instrument/module_{self.modno}\")\n",
    "                g.create_dataset('fit_params', data=self.fit_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = SLURMCluster(\n",
    "    queue='exfel',\n",
    "    processes=16,\n",
    "    cores=16, memory='512GB',\n",
    "    walltime=\"04:00:00\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d47a5fc2d8654d7698601e6f48ce682e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>SLURMCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dask client: <Client: 'tcp://131.169.182.212:34823' processes=64 threads=64, memory=2.05 TB>\n"
     ]
    }
   ],
   "source": [
    "client = Client(cluster)\n",
    "print(\"Created dask client:\", client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/gpfs/exfel/exp/MID/201931/p900091/raw/r0491\"\n",
    "counts_file = \"/home/schroete/test_pixel_dask.h5\"\n",
    "fit_file = \"/home/schroete/fit_dask.h5\"\n",
    "modno = 7\n",
    "bin_edges = np.linspace(-200, 400, 601)\n",
    "pulse_ids = \"1:24:2\"\n",
    "\n",
    "dark_file = os.path.join(\n",
    "    \"/gpfs/exfel/data/scratch/kamile/batch\",\n",
    "    f\"dark_module_{modno}.h5\")\n",
    "\n",
    "histogram_file = os.path.join(\n",
    "    \"/gpfs/exfel/data/scratch/kamile/batch\",\n",
    "    f\"data_module_{modno}.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dark data: (12, 512, 128)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(dark_file, \"r\") as f:\n",
    "    dark_data = f[f\"entry_1/instrument/module_{modno}/image\"][:]\n",
    "\n",
    "print(f\"Shape of dark data: {dark_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "e = EvalHistogram(\n",
    "        modno, path, 'AGIPD', pixel_hist=True)\n",
    "\n",
    "e.process(bin_edges, workers=5, pulse_ids=pulse_ids, dark_run=dark_data)\n",
    "e.hist_to_file(counts_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.perf_counter()\n",
    "params = [100, 70, 50, 10, 10, 10, -25, 25, 70]\n",
    "bounds_minuit = [(0, None), (0, None), (0, None),\n",
    "                (0, None), (0, None), (0, None),\n",
    "                (-50, 0), (0, 50), (40, 100)]\n",
    "\n",
    "print(\"Parameter fertig\")\n",
    "\n",
    "e.fit_histogram(params, bounds_minuit, from_file=histogram_file)\n",
    "\n",
    "print(f\"Time taken for histogram fit.: {time.perf_counter()-t0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.perf_counter()\n",
    "e.fit_params_to_file(fit_file)\n",
    "print(f\"Time taken for writing.: {time.perf_counter()-t0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()\n",
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env]",
   "language": "python",
   "name": "conda-env-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
