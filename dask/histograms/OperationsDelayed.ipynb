{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original at https://git.xfel.eu/gitlab/dataAnalysis/calibration-services/blob/dev/calibration/processor/operations.py\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "from functools import partial\n",
    "import multiprocessing as mp\n",
    "import os.path as osp\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "from iminuit import Minuit\n",
    "from karabo_data import DataCollection, by_index, H5File\n",
    "\n",
    "from fit_functions import least_squares_np\n",
    "from utils import pulse_filter, parse_ids, find_proposal\n",
    "\n",
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from dask import delayed, compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalHistogram:\n",
    "    \"\"\"Class to evaluate histogram\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    modno: (int) Channel number or module number\n",
    "    path: (str) Path to Run folder\n",
    "    dettype: (str) AGIPD, LPD\n",
    "    pixel_hist: (bool) optional\n",
    "        Default: False. For pixel wise histogram set it to True\"\"\"\n",
    "    def __init__(self, modno, path, dettype, pixel_hist=False):\n",
    "        \"\"\"Initialization\"\"\"\n",
    "        self.histograms = None\n",
    "        self.bin_edges = None\n",
    "        self.mean_image = None\n",
    "        self.fit_params = None\n",
    "\n",
    "        self.modno = modno\n",
    "        self.path = path\n",
    "        self.pixel_hist = pixel_hist\n",
    "        self.dettype = dettype\n",
    "        assert self.dettype in [\"AGIPD\", \"LPD\"]\n",
    "\n",
    "    def process(self, bin_edges, pulse_ids=None, dark_run=None):\n",
    "        \"\"\"Evaluate Histogram and mean image\n",
    "        Parameters:\n",
    "        -----------\n",
    "            bin_edges: (np.ndarray) required\n",
    "            pulse_ids: str, optional\n",
    "                Default: all pulses \":\"\n",
    "                For eg. \":\" to select all pulses in a train\n",
    "                \"start:stop:step\" to select indices with certain step size\n",
    "                \"1,2,3\" comma separated pulse index to select specific pulses\n",
    "                \"1,2,3, 5:10\" mix of above two\n",
    "            workers: (int), optional.\n",
    "                Default: half of total cpus available\n",
    "                Distribute sequence files over multiple processors\n",
    "            dark_run: (numpy.ndarray) optional\n",
    "                dark_dta shape (n_pulses, slow_scan, fast_scan)\n",
    "                Default: None,\n",
    "                If provided dark data will be subtracted from images\n",
    "        \"\"\"\n",
    "        self.bin_edges = bin_edges\n",
    "        self.dark_run = dark_run\n",
    "        pulse_ids = \":\" if pulse_ids is None else pulse_ids\n",
    "        self.pulses = parse_ids(pulse_ids)\n",
    "\n",
    "        if not self.path or self.modno not in range(16):\n",
    "            return\n",
    "\n",
    "        pattern = f\"(.+){self.dettype}{self.modno:02d}-S(.+).h5\"\n",
    "\n",
    "        sequences = [osp.join(self.path, f) for f in os.listdir(self.path)\n",
    "                     if f.endswith('.h5') and re.match(pattern, f)]\n",
    "\n",
    "        histograms = []\n",
    "        images = []\n",
    "        \n",
    "        delayedResults = []\n",
    "        \n",
    "                    \n",
    "        for s in sequences:\n",
    "            delayedResults.append(delayed(self._eval)(s))\n",
    "            \n",
    "\n",
    "        results = compute(delayedResults)[0]\n",
    "        \n",
    "        for res in results:\n",
    "            if res[0] is not None and res[1] is not None:\n",
    "                images.append(res[0])\n",
    "                histograms.append(res[1])\n",
    "\n",
    "        if images and histograms:\n",
    "            self.mean_image = np.mean(np.stack(images), axis=0)\n",
    "            self.histograms = sum(histograms)\n",
    "\n",
    "    def _eval(self, seq_file):\n",
    "        \"\"\"Histogram over all or individual pixels\"\"\"\n",
    "        if not seq_file:\n",
    "            return\n",
    "        run = H5File(seq_file).select_trains(by_index[:20])\n",
    "\n",
    "        module = [key for key in run.instrument_sources\n",
    "                  if re.match(r\"(.+)/DET/(.+):(.+)\", key)]\n",
    "\n",
    "        if len(module) != 1:\n",
    "            return\n",
    "\n",
    "        histogram = 0\n",
    "        mean_image = 0\n",
    "        train_counts = 0\n",
    "\n",
    "        for tid, data in run.trains(devices=[(module[0], \"image.data\")],\n",
    "            require_all=True):\n",
    "\n",
    "            image = data[module[0]][\"image.data\"][:, 0, ...]\n",
    "\n",
    "            if image.shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            if self.pulses != [-1]:\n",
    "                image = image[self.pulses, ...].astype(np.float32)\n",
    "            else:\n",
    "                image = image.astype(np.float32)\n",
    "\n",
    "            if self.dark_run is not None:\n",
    "                dark_data = self.dark_run\n",
    "                if image.shape == dark_data.shape:\n",
    "                    image -= dark_data\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        f\"Different data shapes, dark_data: {dark_data.shape}\"\n",
    "                        f\" Run data: {image.shape}\")\n",
    "\n",
    "            mean_image += image\n",
    "            train_counts += 1\n",
    "\n",
    "            if not self.pixel_hist:\n",
    "                \"\"\"Evaluate histogram over entire module\"\"\"\n",
    "                counts_pr = []\n",
    "                \n",
    "                def _eval_stat(pulse):\n",
    "                    counts, _ = np.histogram(\n",
    "                        image[pulse, ...].ravel(), bins=self.bin_edges)\n",
    "                    return counts\n",
    "\n",
    "                        \n",
    "                for i in range(image.shape[0]):\n",
    "                    ret = _eval_stat(i)\n",
    "                    counts_pr.append(ret)\n",
    "                    \n",
    "                histogram += np.stack(counts_pr)\n",
    "\n",
    "            else:\n",
    "                \"\"\"Evaluate histogram over each pixel\"\"\"\n",
    "                \n",
    "                def multihist(chunk, data, bin_edges, ret):\n",
    "                    start, end = chunk\n",
    "                    temp = data[:, start:end, :]\n",
    "                    bin_ix = np.searchsorted(bin_edges[1:], temp)\n",
    "\n",
    "                    X, Y, Z = temp.shape\n",
    "                    xgrid, ygrid, zgrid = np.meshgrid(\n",
    "                        np.arange(X),\n",
    "                        np.arange(Y),\n",
    "                        np.arange(Z),\n",
    "                        indexing='ij')\n",
    "                    \n",
    "                    counts = np.zeros((X, Y, Z, len(bin_edges)), dtype=np.uint32)\n",
    "\n",
    "                    np.add.at(counts, (xgrid, ygrid, zgrid, bin_ix), 1)\n",
    "                    ret[:, start:end, :, :] = counts[..., :-1]\n",
    "                    return counts[..., :-1]\n",
    "                    \n",
    "\n",
    "                counts = np.zeros(\n",
    "                    (len(self.pulses),\n",
    "                    512,\n",
    "                    128,\n",
    "                    len(self.bin_edges)-1),\n",
    "                    dtype=np.uint32)\n",
    "\n",
    "                start = 0\n",
    "                chunk_size = 32\n",
    "                chunks = []\n",
    "\n",
    "                while start < counts.shape[1]:\n",
    "                    chunks.append(\n",
    "                        (start, min(start + chunk_size, counts.shape[1])))\n",
    "                    start += chunk_size              \n",
    "                \n",
    "                for chunk in chunks:\n",
    "                    multihist(chunk, image, self.bin_edges, counts)\n",
    "                \n",
    "                histogram += counts\n",
    "\n",
    "        print(\"Total \", histogram.shape)\n",
    "        if train_counts != 0:\n",
    "            return mean_image / train_counts, histogram\n",
    "\n",
    "    def hist_to_file(self, path):\n",
    "        \"\"\"Write histograms to H5 File\"\"\"\n",
    "        if all([\n",
    "            self.histograms is not None,\n",
    "            self.mean_image is not None,\n",
    "            path]):\n",
    "\n",
    "            bin_centers = (self.bin_edges[1:] + self.bin_edges[:-1]) / 2.0\n",
    "            with h5py.File(path, \"w\") as f:\n",
    "                g = f.create_group(f\"entry_1/instrument/module_{self.modno}\")\n",
    "                g.create_dataset('counts', data=self.histograms)\n",
    "                g.create_dataset('bins', data=bin_centers)\n",
    "                g.create_dataset('image', data=self.mean_image)\n",
    "\n",
    "    def fit_histogram(self, init_params, bounds_params,\n",
    "                      from_file=None, threshold=(-50, 120)):\n",
    "\n",
    "        histogram = self.histograms\n",
    "        bin_edges = self.bin_edges\n",
    "\n",
    "        if bin_edges is not None:\n",
    "            bin_centers = (self.bin_edges[1:] + self.bin_edges[:-1]) / 2.0\n",
    "\n",
    "        self.init_params = init_params\n",
    "        self.bounds_params = bounds_params\n",
    "        if from_file is not None:\n",
    "            with h5py.File(from_file, \"r\") as f:\n",
    "                bin_centers = \\\n",
    "                    f[f\"entry_1/instrument/module_{self.modno}/bins\"][:]\n",
    "                histogram = \\\n",
    "                    f[f\"entry_1/instrument/module_{self.modno}/counts\"][:]\n",
    "\n",
    "        if histogram is None:\n",
    "            return\n",
    "\n",
    "        low, high = threshold\n",
    "        idx = (bin_centers > low) & (bin_centers < high)\n",
    "\n",
    "        hist_for_each = np.split(histogram.flatten(),\n",
    "                                 np.product(histogram.shape[:-1]))\n",
    "\n",
    "        map_fitting = partial(self._fitting, idx, bin_centers)\n",
    "        \n",
    "        print(\"vor ret\")\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "            ret = executor.map(map_fitting, hist_for_each)\n",
    "\n",
    "        #ret = map(map_fitting, hist_for_each)\n",
    "        \n",
    "        print(\"nach ret\")\n",
    "\n",
    "        self.fit_params = np.array(\n",
    "            list(ret)).reshape(\n",
    "            histogram.shape[:-1]+(2*len(self.init_params)+1,))\n",
    "\n",
    "    def _fitting(self, idx, bin_centers, histogram):\n",
    "        least_sq = partial(\n",
    "            least_squares_np,\n",
    "            bin_centers[idx],\n",
    "            histogram[idx])\n",
    "\n",
    "        m = Minuit.from_array_func(\n",
    "            least_sq,\n",
    "            self.init_params,\n",
    "            error=0.1,\n",
    "            errordef=1,\n",
    "            limit=tuple(self.bounds_params))\n",
    "        \n",
    "        \n",
    "        minuit_res = m.migrad()\n",
    "        \n",
    "        \n",
    "        return np.concatenate(\n",
    "            (m.np_values(),\n",
    "             m.np_errors(),\n",
    "             np.array([m.get_fmin().is_valid])))\n",
    "\n",
    "    def fit_params_to_file(self, path):\n",
    "        \"\"\"Write fit params to H5 File\"\"\"\n",
    "        if all([self.fit_params is not None, path]):\n",
    "            with h5py.File(path, \"w\") as f:\n",
    "                g = f.create_group(f\"entry_1/instrument/module_{self.modno}\")\n",
    "                g.create_dataset('fit_params', data=self.fit_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = SLURMCluster(\n",
    "    queue='exfel',\n",
    "    processes=32,\n",
    "    cores=32, memory='512GB',\n",
    "    walltime=\"04:00:00\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af8f2b4f18164189b9d68c4d6b55fbc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>SLURMCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dask client: <Client: 'tcp://131.169.182.98:45339' processes=128 threads=128, memory=2.05 TB>\n"
     ]
    }
   ],
   "source": [
    "client = Client(cluster)\n",
    "print(\"Created dask client:\", client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/gpfs/exfel/exp/MID/201931/p900091/raw/r0491\"\n",
    "counts_file = \"/home/schroete/test_pixelDask.h5\"\n",
    "fit_file = \"/home/schroete/fit.h5\"\n",
    "modno = 7\n",
    "bin_edges = np.linspace(-200, 400, 601)\n",
    "pulse_ids = \"1:24:2\"\n",
    "\n",
    "dark_file = os.path.join(\n",
    "    \"/gpfs/exfel/data/scratch/kamile/batch\",\n",
    "    f\"dark_module_{modno}.h5\")\n",
    "\n",
    "histogram_file = os.path.join(\n",
    "    \"/gpfs/exfel/data/scratch/kamile/batch\",\n",
    "    f\"data_module_{modno}.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dark data: (12, 512, 128)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(dark_file, \"r\") as f:\n",
    "    dark_data = f[f\"entry_1/instrument/module_{modno}/image\"][:]\n",
    "\n",
    "print(f\"Shape of dark data: {dark_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for histogram Eval.: 851.457300064154\n"
     ]
    }
   ],
   "source": [
    "t0 = time.perf_counter()\n",
    "e = EvalHistogram(\n",
    "        modno, path, 'AGIPD', pixel_hist=True)\n",
    "\n",
    "e.process(bin_edges, pulse_ids=pulse_ids, dark_run=dark_data)\n",
    "e.hist_to_file(counts_file)\n",
    "print(f\"Time taken for histogram Eval.: {time.perf_counter()-t0}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.perf_counter()\n",
    "params = [100, 70, 50, 10, 10, 10, -25, 25, 70]\n",
    "bounds_minuit = [(0, None), (0, None), (0, None),\n",
    "                (0, None), (0, None), (0, None),\n",
    "                (-50, 0), (0, 50), (40, 100)]\n",
    "\n",
    "print(\"Parameter fertig\")\n",
    "\n",
    "e.fit_histogram(params, bounds_minuit, from_file=histogram_file)\n",
    "\n",
    "print(f\"Time taken for histogram fit.: {time.perf_counter()-t0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.perf_counter()\n",
    "e.fit_params_to_file(fit_file)\n",
    "print(f\"Time taken for writing.: {time.perf_counter()-t0}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env]",
   "language": "python",
   "name": "conda-env-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
